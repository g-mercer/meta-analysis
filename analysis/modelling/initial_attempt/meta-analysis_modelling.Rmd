---
title: "meta-analysis modelling"
author: "Guy Mercer"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Load packages

```{r}
library(metafor)
library(tidyverse)
library(ape)
library(rotl)
library(corrplot)
library(orchaRd)
library(MuMIn)
library(emmeans)
library(clubSandwich)
```

Update the make_VCV_matrix and vcalc functions to take SE instead of SD and n.

```{r}
vcalc_SE <- function (m, se, p1, data, cov_type = c("ROM", "LOR")) 
{
    cov_type <- match.arg(cov_type, choices = cov_type)
    if (cov_type == "ROM") {
        cov <- data[p1, se]^2/data[p1, m]^2
    }
    if (cov_type == "LOR") {
        cov <- (1/data[p1, m] + 1)/(data[p1, n] - data[p1, m])
    }
    return(cov)
}


make_VCV_matrix_SE <- function (data, V, m, se, n, cluster, obs, type = c("vcv", "cor"), 
    vcal = c("none", "lnOR", "ROM"), rho = 0.5) 
{
    type <- match.arg(type, choices = type)
    vcal <- match.arg(vcal, choices = vcal)
    if (missing(data)) {
        stop("Must specify dataframe via 'data' argument.")
    }
    if (missing(V)) {
        stop("Must specify name of the variance variable via 'V' argument.")
    }
    if (missing(cluster)) {
        stop("Must specify name of the clustering variable via 'cluster' argument.")
    }
    if (missing(obs)) {
        obs <- 1:length(V)
    }
    if (missing(type)) {
        type <- "vcv"
    }
    if (vcal != "none") {
        if (missing(m) | missing(se)) {
            stop("Must specify m and se arguments so that the covariance can be correctly calculated.")
        }
    }
    new_matrix <- matrix(0, nrow = dim(data)[1], ncol = dim(data)[1])
    rownames(new_matrix) <- data[, obs]
    colnames(new_matrix) <- data[, obs]
    tmp <- duplicated(data[, cluster])
    shared_coord <- which(data[, cluster] %in% data[tmp, cluster] == 
        TRUE)
    combinations <- do.call("rbind", tapply(shared_coord, data[shared_coord, 
        cluster], function(x) t(utils::combn(x, 2))))
    if (type == "vcv") {
        for (i in 1:dim(combinations)[1]) {
            p1 <- combinations[i, 1]
            p2 <- combinations[i, 2]
            if (vcal == "none") {
                p1_p2_cov <- rho * sqrt(data[p1, V]) * sqrt(data[p2, 
                  V])
            }
            else {
                p1_p2_cov <- vcalc_SE(m, se, p1, data, cov_type = vcal)
            }
            new_matrix[p1, p2] <- p1_p2_cov
            new_matrix[p2, p1] <- p1_p2_cov
        }
        diag(new_matrix) <- data[, V]
    }
    if (type == "cor") {
        for (i in 1:dim(combinations)[1]) {
            p1 <- combinations[i, 1]
            p2 <- combinations[i, 2]
            p1_p2_cov <- rho
            new_matrix[p1, p2] <- p1_p2_cov
            new_matrix[p2, p1] <- p1_p2_cov
        }
        diag(new_matrix) <- 1
    }
    return(new_matrix)
}
```

Import data. There a 4 variances column. These are based on SE values extracted from 95% CIs under 4 different assumptions:

1) That the t distribution value used to calculate the 95% CI was based on the overall sample size (CONTAINER*INDIVIDUAL/CONTAINER = INDIVIDUAL).
2) That the t distribution value used to calculate the 95% CI was based on an effective sample size derived from an ICC value of 0.046.
3) That the t distribution value used to calculate the 95% CI was based on an effective sample size derived from an ICC value of 0.442.
4) That the t distribution value used to calculate the 95% CI was based on the container level sample size (CONTAINER).

The larger the sample size, the smaller the t distribution value, the larger the SE extracted, the larger the log response ratio variance. The list above is ordered from largest to smallest vi values.

Tidy up variables to get rid of whitespace at the end and get in correct class.

```{r}
ED_50_data <- read.csv("input/lrr_vi.csv")

tidy_up_fun <- function(variable) {
  
  variable <- gsub("\\W+$", "", variable)
  
  variable <- as.factor(variable)
  
}

# tidy up DATA_ID
ED_50_data$DATA_ID <- as.factor(ED_50_data$DATA_ID)

# tidy up STUDY_ID
ED_50_data$STUDY_ID <- tidy_up_fun(variable = ED_50_data$STUDY_ID)

# tidy up MULTIPLE_MEASUREMENT
ED_50_data$MULTIPLE_MEASUREMENT <- tidy_up_fun(variable = ED_50_data$MULTIPLE_MEASUREMENT)

# tidy up AI_NAME
ED_50_data$AI_NAME <- tidy_up_fun(variable = ED_50_data$AI_NAME)

# tidy up AI_ERROR_TYPE
ED_50_data$AI_ERROR_TYPE <- tidy_up_fun(variable = ED_50_data$AI_ERROR_TYPE)

# tidy up FORM_NAME
ED_50_data$FORM_NAME <- tidy_up_fun(variable = ED_50_data$FORM_NAME)

# tidy up FORM_ERROR_TYPE
ED_50_data$FORM_ERROR_TYPE <- tidy_up_fun(variable = ED_50_data$FORM_ERROR_TYPE)

# tidy up COMMON CONTROL
ED_50_data$COMMON_CONTROL <- tidy_up_fun(variable = ED_50_data$COMMON_CONTROL)

# tidy up SPECIES_NAME_BINOMIAL
ED_50_data$SPECIES_NAME_BINOMIAL <- tidy_up_fun(variable = ED_50_data$SPECIES_NAME_BINOMIAL)
ED_50_data$SPECIES_NAME_BINOMIAL <- gsub("^.*microb.*$", "Microbial community", ED_50_data$SPECIES_NAME_BINOMIAL)
ED_50_data$SPECIES_NAME_BINOMIAL <- tidy_up_fun(variable = ED_50_data$SPECIES_NAME_BINOMIAL)

# tidy up SPECIES_CLASS
ED_50_data$SPECIES_CLASS <- tidy_up_fun(variable = ED_50_data$SPECIES_CLASS)
ED_50_data$SPECIES_CLASS <- gsub("Collembola", "Entognatha", ED_50_data$SPECIES_CLASS)
ED_50_data$SPECIES_CLASS <- gsub("Teleostei", "Actinopterygii", ED_50_data$SPECIES_CLASS)
ED_50_data$SPECIES_CLASS <- gsub("Oligochaeta", "Clitellata", ED_50_data$SPECIES_CLASS)
ED_50_data$SPECIES_CLASS <- gsub("n/a", "Microbial community", ED_50_data$SPECIES_CLASS)
ED_50_data$SPECIES_CLASS <- tidy_up_fun(variable = ED_50_data$SPECIES_CLASS)

# tidy up SPECIES_PHYLUM
ED_50_data$SPECIES_PHYLUM <- tidy_up_fun(variable = ED_50_data$SPECIES_PHYLUM)
ED_50_data$SPECIES_PHYLUM <- gsub("n/a", "Microbial community", ED_50_data$SPECIES_PHYLUM)
# different names for the same phylum
ED_50_data$SPECIES_PHYLUM <- gsub("Pseudomonadota", "Proteobacteria", ED_50_data$SPECIES_PHYLUM)
ED_50_data$SPECIES_PHYLUM <- tidy_up_fun(variable = ED_50_data$SPECIES_PHYLUM)

# tidy up SPECIES_KINGDOM
ED_50_data$SPECIES_KINGDOM <- tidy_up_fun(variable = ED_50_data$SPECIES_KINGDOM)
ED_50_data$SPECIES_KINGDOM <- gsub("n/a", "Microbial community", ED_50_data$SPECIES_KINGDOM)
ED_50_data$SPECIES_KINGDOM <- tidy_up_fun(variable = ED_50_data$SPECIES_KINGDOM)

# tidy up PESTICIDE_CLASS
ED_50_data$PESTICIDE_CLASS <- tidy_up_fun(variable = ED_50_data$PESTICIDE_CLASS)
ED_50_data$PESTICIDE_CLASS <- gsub("bactericide/fungicide", "fungicide", ED_50_data$PESTICIDE_CLASS)
ED_50_data$PESTICIDE_CLASS <- tidy_up_fun(variable = ED_50_data$PESTICIDE_CLASS)

# tidy up ECOSYSTEM
ED_50_data$ECOSYSTEM <- tidy_up_fun(variable = ED_50_data$ECOSYSTEM)

# tidy up EXPOSURE_ROUTE
ED_50_data$EXPOSURE_ROUTE <- tidy_up_fun(variable = ED_50_data$EXPOSURE_ROUTE)
ED_50_data$EXPOSURE_ROUTE <- gsub("gavage", "oral-gavage", ED_50_data$EXPOSURE_ROUTE)
ED_50_data$EXPOSURE_ROUTE <- tidy_up_fun(variable = ED_50_data$EXPOSURE_ROUTE)

# tidy up REGULATORY_STANDARDISED_GUIDELINES_FOLLOWED
ED_50_data$REGULATORY_STANDARDISED_GUIDELINES_FOLLOWED <- tidy_up_fun(variable = ED_50_data$REGULATORY_STANDARDISED_GUIDELINES_FOLLOWED)

# tidy up FORMULATION_TYPE
# make singular
ED_50_data$FORMULATION_TYPE <- gsub("s$", "", ED_50_data$FORMULATION_TYPE)
# lowercase
ED_50_data$FORMULATION_TYPE <- tolower(ED_50_data$FORMULATION_TYPE)
# factor
ED_50_data$FORMULATION_TYPE <- as.factor(ED_50_data$FORMULATION_TYPE)
```

In the chunk below I remove all LRR_VAR_INDIVIDUAL values > 10, which corresponds to effect sizes where one of the AI or FORM had a CI arm >10 times the size of the point estimate. This (I think) is indicative of a poor model fit in the primary source. This removes 6 effect sizes.

If I don't do this max(vi) / min(vi) >= 1e7 and the model throws a warning message saying "Warning: Ratio of largest to smallest sampling variance extremely large. May not be able to obtain stable results".

[Discussion on large Vi range](https://stat.ethz.ch/pipermail/r-sig-meta-analysis/2019-February/001426.html)

```{r}
ED_50_data_red <- ED_50_data [ED_50_data$LRR_VAR_INDIVIDUAL < 10, ]
```

I also remove microbial community effect sizes as a phylogenetic correlation can't be calculated for these and save in a separate df.

```{r}
# remove microbial community rows
ED_50_data_phylo <- ED_50_data_red [ED_50_data_red$SPECIES_NAME_BINOMIAL != "Microbial community", ]

# removes unused level that was dropped when microbial community was filtered out
ED_50_data_phylo$FORMULATION_TYPE <- factor(ED_50_data_phylo$FORMULATION_TYPE)
```

Phylogenetic reconstruction at the species level

```{r}
library(rotl)

taxa <- unique(as.character(ED_50_data_phylo$SPECIES_NAME_BINOMIAL))

# resolves names. A few of the names I had were older synonyms
resolved_names <- tnrs_match_names(taxa)

my_tree <- tol_induced_subtree(ott_ids = resolved_names$ott_id, label_format = "name")

plot(my_tree, type = "phylogram", cex = 0.3)

# add to df OTL species names
name_update <- cbind(unique(as.character(ED_50_data_phylo$SPECIES_NAME_BINOMIAL)), resolved_names$unique_name)

ED_50_data_phylo$SPECIES_NAME_BINOMIAL_OTL <- ""

for (i in 1:nrow(name_update)) {
  
  for (j in 1:nrow(ED_50_data_phylo)) {
  
    if (name_update [i, 1] == ED_50_data_phylo$SPECIES_NAME_BINOMIAL [j]) {
    
    ED_50_data_phylo$SPECIES_NAME_BINOMIAL_OTL [j] <- name_update [i, 2]
    
    }
  
  }

}

# give updated names underscores to match species names in tree
ED_50_data_phylo$SPECIES_NAME_BINOMIAL_OTL <- gsub(" ", "_", ED_50_data_phylo$SPECIES_NAME_BINOMIAL_OTL)

tree_tip_label <- my_tree$tip.label
updated_species_list <- unique(ED_50_data_phylo$SPECIES_NAME_BINOMIAL_OTL)

# check if names from the raw data are not found in tree. Match up.
setdiff(tree_tip_label, updated_species_list)

library(ape)

# calculate branch lengths
my_tree <- ape::compute.brlen(my_tree, method = "Grafen", power = 1)

# use a randomization approach to deal with polytomies
my_tree <- ape::multi2di(my_tree, random = TRUE)

# create correlation matrix for analysis
phylo_cor <- vcv(my_tree, cor = T)
```

Make the M matrix for the VAR_INDIVIDUAL level values.

```{r}
# for phylo
ED_50_data_phylo$obs <- 1:dim(ED_50_data_phylo)[1]

M_phylo <- make_VCV_matrix_SE(data = ED_50_data_phylo, V = "LRR_VAR_INDIVIDUAL", cluster = "COMMON_CONTROL", m = "AI_50",
                        se = "AI_SE_INDIVIDUAL", type = "vcv", vcal = "ROM", obs = "obs")

# change the indexing to look over the whole dataset. Increment of 100 just about works. 10 is better to see small correlations.
library(corrplot)
corrplot::corrplot(cov2cor(M_phylo) [320:330, 320:330])
```

These seems to be the most updated guides

[Nagakawa - guide](https://environmentalevidencejournal.biomedcentral.com/articles/10.1186/s13750-023-00301-6#Sec13)
[Nagakawa - guide associated tutorial](https://itchyshin.github.io/Meta-analysis_tutorial/)
[Nagakawa - publication bias](https://itchyshin.github.io/publication_bias/#Appendix_extra:_Figure%E2%80%99s_R_code)

Here's my list of potential random effects:

1) ~1 | STUDY (between study variation)

2) ~1 | DATA_ID (within study variation (residual variance))

3) ~1 | SPECIES_NAME_BINOMIAL_OTL (control for evolutionary relatedness. For example, co-formulants are expected to affect two species of water flea more                                    similarly than a bacteria due to more similar molecular and cellular targets, due to a shared evolutionary history)

4) ~1 | SPECIES_NAME_BINOMIAL (the non-phylogenetic relatedness, this controls for ecological similarities like being both aquatic. Failing to include this term will often inflate the phylogenetic variance)

5) A way to capture MULTIPLE_MEASUREMENTS, through time.

For my meta-analysis MULTIPLE_MEASUREMENT indicates where multiple ED50s were extracted from the same group of organisms. For example, a group of organisms were treated with the ai or formulation at t=0. Then, ED50s were calculated at 24,48 and 96h using the same group of organisms. This is time so will require some auto-correlation structure. The simplest auto-correlation structure is the compound symmetry structure. This assumes that whatever the distance in time between two observations, their residual correlation is the same. Compound symmetry structure is the correlation structure imposed by random intercept models.

I will try 3 approaches of increasing complexity

i) ~1 | MULTIPLE_MEASUREMENT
ii) TIME | MULTIPLE_MEASUREMENT, struct = "AR"
iii) TIME | MULTIPLE_MEASUREMENT, structure = "CAR"

i, this correlation structure is often too simplistic for time series, but may still be useful for short time series. All of my time series have <5 time points apart from 2, which have 7. Therefore, it may suffice. The difference between ii and iii is AR assumes that time points are evenly spaced where CAR does not. AR may be enough to capture the time correlation, even if it isn't strictly true.

***The three approaches above induce dependence among the effect sizes but this is a 'shared measurement' problem as well so it will induce dependence among the sampling variances . I think it is as well because [this example](https://wviechtb.github.io/metadat/reference/dat.fine1993.html) has the same data structure as me and used both a sampling variance vcv matrix and a random effect. Also [this example](https://wviechtb.github.io/metadat/reference/dat.knapp2017.html) says, "All 4 issues described above lead to a multilevel structure in the dataset, with multiple standardized mean differences nested within some of the studies. Issues 1. and 2. also lead to correlated sampling errors."**

There are two ways to tackle non-independence among sampling variances due to 'shared measurement'. 

a) The first is to model the dependence using a variance-covariance matrix like I did for the common control. Unlike the common control scenario a correlation value, 𝜌, has to be supplied. We don't know what this should be so usually 0.5-0.8 is used. I assume this would be supplied to the rma.mv in the same way as the common control matrix so I can't do both the 'common control' and 'shared measurement'.

b) Use Robust Variance Estimation (RVE) to overcome the unknown 𝜌 issue by approximating average dependence among sampling variance (and effect sizes) from the data and incorporating such dependence to estimate standard errors.

Currently we don't know which of these approaches is better as simulations haven't been performed. "For now, one could use both VCV matrices and RVE in the same model". So I could use the common control VCV matrix with RVE.

Quote from later in the article from the section "Complex non-independence", "Statistically, spatial correlation can be also modelled in a manner analogous to phylogenetic relatedness (i.e., rather than a phylogenetic correlation matrix, A, we fit a spatial correlation matrix). For example, Maire and colleagues [114] used a meta-analytic model with spatial autocorrelation to investigate the temporal trends of fish communities in the network of rivers in France. We note that a similar argument can be made for temporal correlation, but in many cases, temporal correlations could be dealt with, albeit less accurately, as a special case of ‘shared measurements’".

Also, I have to adjust the sampling variance to account for common controls.

I can use Robust Variance Estimation to see if I have adequately captured all forms of non-independence.

I can make a matrix that accounts for common control and shared measurement at the [same time](https://wviechtb.github.io/metadat/reference/dat.knapp2017.html)

There is a model selection section in the associated tutorial. What they say contradicts Zuur and other sources regarding model selection of random effects. They said that you should always change to "ML" when you only have to when performing model selection on fixed effects. REML is actually slightly better when looking at random effects while fixed effects remain constant.

Their guides seems to do a simple form of forward selection. I'll perform forward selection as in Zuur. Pick the most significant term, then add and continue, building the model up.

Let's see which of these random effects improve model fit. 

Starting with a NULL model with all the potential fixed effects and the common control vcv matrix.

```{r}
library(metafor)

# NULL model
null_mod <- rma.mv(yi = LRR, V = M_phylo,
                    mod = ~ PESTICIDE_CLASS + EXPOSURE_ROUTE + FORMULATION_TYPE,
                    data = ED_50_data_phylo, method = "REML", test = "t", dfs = "contain")

# ROUND 1
# add STUDY_ID as random effect
study_mod <- rma.mv(yi = LRR, V = M_phylo,
                    mod = ~ PESTICIDE_CLASS + EXPOSURE_ROUTE + FORMULATION_TYPE,
                    random = list(~1 | STUDY_ID),
                    data = ED_50_data_phylo, method = "REML", test = "t", dfs = "contain")

# STUDY_ID is significant
anova.rma(null_mod, study_mod)

# add DATA_ID as a random effect
es_mod <- rma.mv(yi = LRR, V = M_phylo,
                    mod = ~ PESTICIDE_CLASS + EXPOSURE_ROUTE + FORMULATION_TYPE,
                    random = list(~1 | DATA_ID),
                    data = ED_50_data_phylo, method = "REML", test = "t", dfs = "contain")

# DATA_ID is significant
anova.rma(null_mod, es_mod)

# phylogenetic species term SPECIES_NAME_BINOMIAL_OTL
phylo_mod <- rma.mv(yi = LRR, V = M_phylo,
                    mod = ~ PESTICIDE_CLASS + EXPOSURE_ROUTE + FORMULATION_TYPE,
                    random = list(~1 | SPECIES_NAME_BINOMIAL_OTL),
                    R = list(SPECIES_NAME_BINOMIAL_OTL = phylo_cor),
                    data = ED_50_data_phylo, method = "REML", test = "t", dfs = "contain")

# significant
anova.rma(null_mod, phylo_mod)

# non-phylogenetic species term SPECIES_NAME_BINOMIAL_OTL
species_mod <- rma.mv(yi = LRR, V = M_phylo,
                    mod = ~ PESTICIDE_CLASS + EXPOSURE_ROUTE + FORMULATION_TYPE,
                    random = list(~1 | SPECIES_NAME_BINOMIAL),
                    data = ED_50_data_phylo, method = "REML", test = "t", dfs = "contain")

# significant
anova.rma(null_mod, species_mod)

# ROUND 2, DATA_ID is now the reference
# DATA_ID + STUDY_ID
es_study_mod <- rma.mv(yi = LRR, V = M_phylo,
                    mod = ~ PESTICIDE_CLASS + EXPOSURE_ROUTE + FORMULATION_TYPE,
                    random = list(~1 | DATA_ID,
                                  ~1 | STUDY_ID),
                    data = ED_50_data_phylo, method = "REML", test = "t", dfs = "contain")

# significant
anova.rma(es_mod, es_study_mod)

es_phylo_mod <- rma.mv(yi = LRR, V = M_phylo,
                    mod = ~ PESTICIDE_CLASS + EXPOSURE_ROUTE + FORMULATION_TYPE,
                    random = list(~1 | DATA_ID,
                                  ~1 | SPECIES_NAME_BINOMIAL_OTL),
                    R = list(SPECIES_NAME_BINOMIAL_OTL = phylo_cor),
                    data = ED_50_data_phylo, method = "REML", test = "t", dfs = "contain")

# significant
anova.rma(es_mod, es_phylo_mod)

es_species_mod <- rma.mv(yi = LRR, V = M_phylo,
                    mod = ~ PESTICIDE_CLASS + EXPOSURE_ROUTE + FORMULATION_TYPE,
                    random = list(~1 | DATA_ID,
                                  ~1 | SPECIES_NAME_BINOMIAL),
                    data = ED_50_data_phylo, method = "REML", test = "t", dfs = "contain")

# significant
anova.rma(es_mod, es_species_mod)

# ROUND 3 DATA_ID + STUDY_ID is now the reference
es_study_phylo_mod <- rma.mv(yi = LRR, V = M_phylo,
                    mod = ~ PESTICIDE_CLASS + EXPOSURE_ROUTE + FORMULATION_TYPE,
                    random = list(~1 | DATA_ID,
                                  ~1 | STUDY_ID,
                                  ~1 | SPECIES_NAME_BINOMIAL_OTL),
                    R = list(SPECIES_NAME_BINOMIAL_OTL = phylo_cor),
                    data = ED_50_data_phylo, method = "REML", test = "t", dfs = "contain")

# not significant
anova(es_study_mod, es_study_phylo_mod)

es_study_species_mod <- rma.mv(yi = LRR, V = M_phylo,
                    mod = ~ PESTICIDE_CLASS + EXPOSURE_ROUTE + FORMULATION_TYPE,
                    random = list(~1 | DATA_ID,
                                  ~1 | STUDY_ID,
                                  ~1 | SPECIES_NAME_BINOMIAL),
                    data = ED_50_data_phylo, method = "REML", test = "t", dfs = "contain")

# not significant
anova(es_study_mod, es_study_species_mod)

library(orchaRd)

# supports the likelihood ratio test by showing that variation between species very small
i2_ml(es_study_species_mod)

# supports the likelihood ratio test by showing that variation between phylo-species very small
i2_ml(es_study_phylo_mod)

# this is the final model in relation to the random effects, if we capture
# the 'shared measurement' using RVE later on
es_study_mod <- rma.mv(yi = LRR, V = M_phylo,
                    mod = ~ PESTICIDE_CLASS + EXPOSURE_ROUTE + FORMULATION_TYPE,
                    random = list(~1 | DATA_ID,
                                  ~1 | STUDY_ID),
                    data = ED_50_data_phylo, method = "REML", test = "t", dfs = "contain")

summary(es_study_mod)

```

AR correlation to model the dependence between effect sizes due to 'shared measurements'. Does not tackle dependence also introduced to sampling variance.

First create a SIMPLE_TIME variable to correctly model AR.

```{r}
mult_meas_cluster <- unique(ED_50_data_phylo$MULTIPLE_MEASUREMENT)

cluster_list <- vector(mode='list', length=length(mult_meas_cluster))

for (i in 1:length(mult_meas_cluster)) {
  
    for (j in 1:nrow(ED_50_data_phylo)) {
    
    if (mult_meas_cluster [i] == ED_50_data_phylo$MULTIPLE_MEASUREMENT [j]) {
      
      cluster_list [[i]] <- rbind(cluster_list [[i]], ED_50_data_phylo [j, ])
      
    }
      
  }
  
}

for (i in 1:length(cluster_list)) {
  
  cluster_list [[i]]$TIME_SIMPLE <- 0
  
  for (j in 1:nrow(as.data.frame(cluster_list [i]))) {
    
    cluster_list [[i]][j, colnames(cluster_list [[i]]) == "TIME_SIMPLE"] <- j
    
  }
}

# turn back into df
library(tidyverse)

ED_50_data_simple_time <- tibble()

for (i in 1:length(cluster_list)) {
  
  ED_50_data_simple_time <- rbind(ED_50_data_simple_time, cluster_list [[i]])
  
}

# ED_50_data_simple_time$TIME_SIMPLE to ED_50_data_phylo according to DATA_ID

ED_50_data_phylo$TIME_SIMPLE <- 0

for (i in 1:nrow(ED_50_data_phylo)) {
  
  for (j in 1:nrow(ED_50_data_simple_time)) {
    
    if (ED_50_data_phylo$DATA_ID [i] == ED_50_data_simple_time$DATA_ID [j]) {
      
      ED_50_data_phylo$TIME_SIMPLE [i] <- ED_50_data_simple_time$TIME_SIMPLE [j]
      
    }
    
  }
  
}

ED_50_data_phylo <- ED_50_data_phylo [, c(1:3, 52, 4:51)]
```

Now model using AR

```{r}
# add an AR correlation to capture the dependence between the effect sizes.
# Can't model the dependence among sampling variances introduced by 'shared measurements'
# at the same time as the 'common control'. Would have to supply two vcv matrices to V
# unless I can combine them into one?

# use TIME_SIMPLE to see if this way of specifying the model alters anything
es_study_ar_mod <- rma.mv(yi = LRR, V = M_phylo,
                    mod = ~ PESTICIDE_CLASS + EXPOSURE_ROUTE + FORMULATION_TYPE,
                    random = list(~1 | DATA_ID,
                                  ~1 | STUDY_ID,
                                  ~ TIME_SIMPLE | MULTIPLE_MEASUREMENT),
                    struct = "AR",
                    data = ED_50_data_phylo, method = "REML", test = "t", dfs = "contain")

anova(es_study_mod, es_study_ar_mod)

summary(es_study_ar_mod)

# a CAR model where timepoint aren't assumed to be evenly spaced is worse.
es_study_car_mod <- rma.mv(yi = LRR, V = M_phylo,
                    mod = ~ PESTICIDE_CLASS + EXPOSURE_ROUTE + FORMULATION_TYPE,
                    random = list(~1 | DATA_ID,
                                  ~1 | STUDY_ID,
                                  ~ EXPOSURE_DURATION_HOURS | MULTIPLE_MEASUREMENT),
                    struct = "CAR",
                    data = ED_50_data_phylo, method = "REML", test = "t", dfs = "contain")

anova(es_study_mod, es_study_car_mod)

# the AR model is the best
AIC(es_study_mod, es_study_ar_mod, es_study_car_mod)
```

What does this tell me? The between study variance is represented by STUDY_ID and the within study variance is now represented by MULTIPLE_MEASUREMENT. All my moderators can be thought of as at the MULTIPLE_MEASUREMENT level. Ie, FORMULATION_TYPE is the same when MULTIPLE_MEASUREMENT is the same.

Model Selection (using the incomplete variance covariance sampling matrix)

```{r}
# express using ML
es_study_ar_mod <- rma.mv(yi = LRR, V = M_phylo,
                    mod = ~ PESTICIDE_CLASS + EXPOSURE_ROUTE + FORMULATION_TYPE,
                    random = list(~1 | DATA_ID,
                                  ~1 | STUDY_ID,
                                  ~ TIME_SIMPLE | MULTIPLE_MEASUREMENT),
                    struct = "AR",
                    data = ED_50_data_phylo, method = "ML", test = "t", dfs = "contain")

es_study_ar_mod_a <- rma.mv(yi = LRR, V = M_phylo,
                    mod = ~ PESTICIDE_CLASS + EXPOSURE_ROUTE,
                    random = list(~1 | DATA_ID,
                                  ~1 | STUDY_ID,
                                  ~ TIME_SIMPLE | MULTIPLE_MEASUREMENT),
                    struct = "AR",
                    data = ED_50_data_phylo, method = "ML", test = "t", dfs = "contain")

es_study_ar_mod_b <- rma.mv(yi = LRR, V = M_phylo,
                    mod = ~ PESTICIDE_CLASS + FORMULATION_TYPE,
                    random = list(~1 | DATA_ID,
                                  ~1 | STUDY_ID,
                                  ~ TIME_SIMPLE | MULTIPLE_MEASUREMENT),
                    struct = "AR",
                    data = ED_50_data_phylo, method = "ML", test = "t", dfs = "contain")

es_study_ar_mod_c <- rma.mv(yi = LRR, V = M_phylo,
                    mod = ~ EXPOSURE_ROUTE + FORMULATION_TYPE,
                    random = list(~1 | DATA_ID,
                                  ~1 | STUDY_ID,
                                  ~ TIME_SIMPLE | MULTIPLE_MEASUREMENT),
                    struct = "AR",
                    data = ED_50_data_phylo, method = "ML", test = "t", dfs = "contain")

anova(es_study_ar_mod, es_study_ar_mod_a)
anova(es_study_ar_mod, es_study_ar_mod_b)
anova(es_study_ar_mod, es_study_ar_mod_c)

# es_study_ar_mod_b now reference
es_study_ar_mod_b_a <- rma.mv(yi = LRR, V = M_phylo,
                    mod = ~ PESTICIDE_CLASS,
                    random = list(~1 | DATA_ID,
                                  ~1 | STUDY_ID,
                                  ~ TIME_SIMPLE | MULTIPLE_MEASUREMENT),
                    struct = "AR",
                    data = ED_50_data_phylo, method = "ML", test = "t", dfs = "contain")

es_study_ar_mod_b_b <- rma.mv(yi = LRR, V = M_phylo,
                    mod = ~ FORMULATION_TYPE,
                    random = list(~1 | DATA_ID,
                                  ~1 | STUDY_ID,
                                  ~ TIME_SIMPLE | MULTIPLE_MEASUREMENT),
                    struct = "AR",
                    data = ED_50_data_phylo, method = "ML", test = "t", dfs = "contain")

anova(es_study_ar_mod_b, es_study_ar_mod_b_a)
anova(es_study_ar_mod_b, es_study_ar_mod_b_b)

# es_study_ar_mod_b_a reference
es_study_ar_mod_b_a_a <- rma.mv(yi = LRR, V = M_phylo,
                    mod = ~ 1,
                    random = list(~1 | DATA_ID,
                                  ~1 | STUDY_ID,
                                  ~ TIME_SIMPLE | MULTIPLE_MEASUREMENT),
                    struct = "AR",
                    data = ED_50_data_phylo, method = "ML", test = "t", dfs = "contain")

anova(es_study_ar_mod_b_a, es_study_ar_mod_b_a_a)

# using MuMIn
library(MuMIn)

eval(metafor:::.MuMIn) # use eval() function to extract helper functions from MuMIn and make them usable in metafor.

mod.candidate <- dredge(es_study_ar_mod, beta = "none", evaluate = TRUE, rank = "AICc", trace=2) # dredge to produce all possible models

# same as manual selection
subset(mod.candidate, delta <= 2, recalc.weights = FALSE)
```

So my final_model (at the moment) is this:

```{r}
final_model <- rma.mv(yi = LRR, V = M_phylo,
                    mod = ~ 0 + PESTICIDE_CLASS,
                    random = list(~1 | DATA_ID,
                                  ~1 | STUDY_ID,
                                  ~ TIME_SIMPLE | MULTIPLE_MEASUREMENT),
                    struct = "AR",
                    data = ED_50_data_phylo, method = "REML", test = "t", dfs = "contain")

summary(final_model)

# RVE causes the fungicide to change alot. I think this is because of the two outliers.
# formally investigate to see if these two outliers are indeed changing the results.
robust(final_model, cluster = ED_50_data_phylo$STUDY_ID, clubSandwich = TRUE)
```

Have a look at an intercept only model. So overall there is an effect, but this is being driven by herbicides.

```{r}
final_model_intercept <- rma.mv(yi = LRR, V = M_phylo,
                    mod = ~ 1,
                    random = list(~1 | DATA_ID,
                                  ~1 | STUDY_ID,
                                  ~ TIME_SIMPLE | MULTIPLE_MEASUREMENT),
                    struct = "AR",
                    data = ED_50_data_phylo, method = "REML", test = "t", dfs = "contain")

summary(final_model_intercept)

# RVE causes the fungicide to change alot. Shows I'm not capturing something in the
# sampling variance dependence or random effects structure.
robust(final_model_intercept, cluster = ED_50_data_phylo$STUDY_ID, clubSandwich = TRUE)

# simple model intercept only. For rough CV calculation
final_model_simple_intercept <- rma.mv(yi = LRR, V = M_phylo,
                    mod = ~ 1,
                    random = list(~1 | STUDY_ID/MULTIPLE_MEASUREMENT/DATA_ID),
                    data = ED_50_data_phylo, method = "REML", test = "t", dfs = "contain")

summary(final_model_simple_intercept)
```

In the measuring heterogeneity of [shinichi guide](https://itchyshin.github.io/Meta-analysis_tutorial/#ref-midolo2019global) they quote,

"In this worked example ([1]), the variation in the true effects (τ2+σ2) is 0.0706, which is quite large given the magnitude of the overall effect (β0
 = 0.0297). Put differently, true effects’ heterogeneity is more than twice the magnitude of true effects (CV = 2.37)"
 
 Using the final_model_simple_intercept to get a rough idea of this, overall intercept = -0.5275 and total heterogeneity =  1.2992 + 1.1816 + 0.0135 = 2.4943. 2.4943/0.5275 = 4.728531. So the heterogeneity of the true effects is almost five times the magnitude of true effects. If 2.37 was quite large then I'm 4.72 is large/very large!

Explore Heterogeneity. Now an issue I am having is orchaRd doesn't work with models with heterogeneous variance. Can use the simpler model with STUDY_ID/MULTIPLE_MEASUREMENT/DATA_ID

```{r}
# can run them using the simpler model
simple_mod_selected <- rma.mv(yi = LRR, V = M_phylo,
                    mod = ~  0 + PESTICIDE_CLASS,
                    random = list(~1 | STUDY_ID/MULTIPLE_MEASUREMENT/DATA_ID),
                    data = ED_50_data_phylo, method = "REML", test = "t", dfs = "contain")

summary(simple_mod_selected)

robust(simple_mod_selected, cluster = ED_50_data_phylo$STUDY_ID)

i2_ml(simple_mod_selected)

r2_ml(simple_mod_selected)
```

Pesticide class explains 6.25% of the variation between effect sizes. Sampling variance accounts for 0.144423%, between study variance for 48.5815812% and within study variance 50.6976173% + 0.5763785% = 51.274% (all roughly).

Explore heterogeneity with prediction intervals. Shows there is a lot of heterogeneity and 95% of the time new effect size estimates will vary from -3.68 to 2.63.

```{r}
predict(final_model_intercept)
```

Orchard Plot.

```{r}
orchard_plot(final_model, 
             mod = "PESTICIDE_CLASS", 
             xlab = "Effect size lnRR", 
             group = "STUDY_ID",  k = TRUE, g = TRUE, trunk.size = 1.5)
```

The orchard plot shows that there are two fungicide effect sizes that appear to be outliers. I think these are resulting in the difference when models are run using RVE.

Attempt leave out one analysis to confirm that these effect sizes are skewing the results.

```{r}
# refactor DATA_ID
ED_50_data_phylo$DATA_ID <- factor(ED_50_data_phylo$DATA_ID)

leave1out_mod <- list()
leave1out_VCV <- list()

for (i in 1:length(ED_50_data_phylo$DATA_ID)) {
  
  message(paste0("Now running model ", ED_50_data_phylo$DATA_ID [i], sep = ""))
  
  dat <- ED_50_data_phylo [ED_50_data_phylo$DATA_ID != ED_50_data_phylo$DATA_ID [i], ]
  
  # have to edit this when I get to the sampling variance covariance matrix based on MULTIPLE_MEASUREMENT
  leave1out_VCV [[i]] <- make_VCV_matrix_SE(data = dat, V = "LRR_VAR_INDIVIDUAL", cluster = "COMMON_CONTROL", m = "AI_50",
                        se = "AI_SE_INDIVIDUAL", type = "vcv", vcal = "ROM", obs = "obs")
  
  leave1out_mod [[i]] <- rma.mv(yi = LRR, V = leave1out_VCV [[i]],
                               mod = ~ 1,
                               random = list(~1 | DATA_ID,
                                             ~1 | STUDY_ID,
                                             ~ TIME_SIMPLE | MULTIPLE_MEASUREMENT),
                               struct = "AR",
                               data = dat, method = "REML", test = "t", dfs = "contain")
  
}

est.func <- function(mod) {
  
  df <- data.frame(est = mod$b, lower = mod$ci.lb, upper = mod$ci.ub)
  
  return(df)
  
}

leave1out_results <- lapply(leave1out_mod, function(x) est.func(x)) %>% bind_rows %>% mutate(DATA_ID = ED_50_data_phylo$DATA_ID)

write.csv(file = "./output/effect_size_level_leave1out.csv", x = leave1out_results, row.names = FALSE)
```

Would be good to edit the above to run with PESTICIDE_CLASS as a moderator and show how leaving out each effect size affects the PESTICIDE_CLASS estimates.

```{r}
# refactor DATA_ID
ED_50_data_phylo$DATA_ID <- factor(ED_50_data_phylo$DATA_ID)

leave1out_mod <- list()
leave1out_VCV <- list()

for (i in 1:length(ED_50_data_phylo$DATA_ID)) {
  
  message(paste0("Now running model ", ED_50_data_phylo$DATA_ID [i], sep = ""))
  
  dat <- ED_50_data_phylo [ED_50_data_phylo$DATA_ID != ED_50_data_phylo$DATA_ID [i], ]
  
  # have to edit this when I get to the sampling variance covariance matrix based on MULTIPLE_MEASUREMENT
  leave1out_VCV [[i]] <- make_VCV_matrix_SE(data = dat, V = "LRR_VAR_INDIVIDUAL", cluster = "COMMON_CONTROL", m = "AI_50",
                        se = "AI_SE_INDIVIDUAL", type = "vcv", vcal = "ROM", obs = "obs")
  
  leave1out_mod [[i]] <- rma.mv(yi = LRR, V = leave1out_VCV [[i]],
                               mod = ~ 0 + PESTICIDE_CLASS,
                               random = list(~1 | DATA_ID,
                                             ~1 | STUDY_ID,
                                             ~ TIME_SIMPLE | MULTIPLE_MEASUREMENT),
                               struct = "AR",
                               data = dat, method = "REML", test = "t", dfs = "contain")
  
}

est.func <- function(mod) {
  
  df <- data.frame(est = mod$b, lower = mod$ci.lb, upper = mod$ci.ub)
  
  df <- matrix(t(as.matrix(df)), nrow = 1)
  
  df <- as.data.frame(df)
  
  return(df)
  
}

leave1out_results_moderator_level <- lapply(leave1out_mod, function(x) est.func(x)) %>% bind_rows %>% mutate(DATA_ID = ED_50_data_phylo$DATA_ID)

colnames(leave1out_results_moderator_level) <- c("alg_est", "alg_lower", "alg_upper",
                                 "fung_est", "fung_lower", "fung_upper",
                                 "herb_est", "herb_lower", "herb_upper",
                                 "ins_est", "ins_lower", "ins_upper",
                                 "DATA_ID")

# this shows that leaving out DATA_ID 409 and 410 changes the fungicide result from significant to non-significant.
# influential effect sizes and outliers.
leave1out_results_moderator_level [leave1out_results_moderator_level$fung_upper > 0, ]

# save output
write_csv("output/effect_size_level_leave1out_by_moderator_level.csv", leave1out_results_moderator_level)
```

Remove 409 and 410, fit the final model again and compare to RVE estimate.

```{r}
ED_50_data_phylo_no_outliers <- ED_50_data_phylo [(ED_50_data_phylo$DATA_ID != "409") & (ED_50_data_phylo$DATA_ID != "410"), ]

M_phylo_no_outliers <- make_VCV_matrix_SE(data = ED_50_data_phylo_no_outliers, V = "LRR_VAR_INDIVIDUAL", cluster = "COMMON_CONTROL", m = "AI_50",
                        se = "AI_SE_INDIVIDUAL", type = "vcv", vcal = "ROM", obs = "obs")


final_model_no_outliers <- rma.mv(yi = LRR, V = M_phylo_no_outliers,
                    mod = ~ 0 + PESTICIDE_CLASS,
                    random = list(~1 | DATA_ID,
                                  ~1 | STUDY_ID,
                                  ~ TIME_SIMPLE | MULTIPLE_MEASUREMENT),
                    struct = "AR",
                    data = ED_50_data_phylo_no_outliers, method = "REML", test = "t", dfs = "contain")

summary(final_model_no_outliers)

# this is much better agreement with the model. Test of the moderators is now not significant
# but herbicide is significant. Algaecide CI becomes very wide but I think that is because they are
# all from one study.
robust(final_model_no_outliers, cluster = ED_50_data_phylo_no_outliers$STUDY_ID, clubSandwich = TRUE)
```

Trying a different correlation structure of the effect size **estimates**. Variance covariance matrix for the sampling variance. Specify a correlation structure with a "sampling auto-correlation" assumption.

I used these sources:

[Pustejovsky & Tipton 2021](https://link.springer.com/article/10.1007/s11121-021-01246-3). This reference explains how using RVE guards against misspecification. They recommend (1) choosing an assumption for the within-study correlation between effect size estimates, (2) identifying a structure for the random effects, and (3) determining whether to include any additional levels (like MULTIPLE_MEASUREMENT). I have done all this but not in the same order because I realised (3) had to be done when I was extracting my data, I was familiar with (2) from my previous experience with mixed effects models and I have struggled with implementing (1) beyond the shared control. I think I am going to have to pick either the shared control approach of AR approach for the effect size estimate correlation structure.

Here is a useful excerpt from the above paper,

"Although RVE standard errors are statistically valid under any set of weights, the choice of weights does impact the precision of the meta-regression coefficient estimator (βˆ). The most precise estimator results when the weights are exactly inverse of the true variance–covariance matrix. However, because the true covariance matrix Φ𝑗 is unknown, we must in practice use a working model—meaning a good guess or rough approximation—for purposes of developing weight matrices. If the working model is correct, the resulting weights are exactly inverse variance, and the meta-regression estimator is fully efficient. If the working model is only close to correct, the resulting weights are still approximately inverse variance, and the meta-regression estimator is still close to efficient. In contrast, if the working model is quite discrepant from the true covariance structure, the meta-regression estimator may be much less efficient, although it is still unbiased and inferences based on RVE remain statistically valid. Thus, the choice of working model and associated weight matrices offer a means to improve the precision of meta-regression coefficient estimates."

So it is worth spending time trying to get an accurate var-cov matrix for the effect size estimates/sampling variance.

[impute_covariance_matrix](https://rdrr.io/cran/clubSandwich/man/impute_covariance_matrix.html) is one option but I ended up using vcalc

```{r}
autocorrelation_matrix <- vcalc(vi = ED_50_data_phylo_no_outliers$LRR_VAR_INDIVIDUAL,
               cluster = MULTIPLE_MEASUREMENT,
               time1 = TIME_SIMPLE,
               phi=0.8,
               data=ED_50_data_phylo_no_outliers)

# shows the pattern I want of decreasing correlation as effect sizes become 
# further away in time.
# corrplot::corrplot(cov2cor(autocorrelation_matrix) [100:150, 100:150])

# fit the final model with this autocorrelation matrix. No warning message.
final_model_no_outliers_autocorrelation_matrix <- rma.mv(yi = LRR, V = autocorrelation_matrix,
                    mod = ~ 0 + PESTICIDE_CLASS,
                    random = list(~1 | DATA_ID,
                                  ~1 | STUDY_ID,
                                  ~ TIME_SIMPLE | MULTIPLE_MEASUREMENT),
                    struct = "AR",
                    data = ED_50_data_phylo_no_outliers, method = "REML", test = "t", dfs = "contain")

summary(final_model_no_outliers_autocorrelation_matrix)

# are the CIs closer than before?
robust(final_model_no_outliers_autocorrelation_matrix, cluster = ED_50_data_phylo_no_outliers$STUDY_ID, clubSandwich = TRUE)

profile(final_model_no_outliers_autocorrelation_matrix, sigm2=1, progbar=FALSE) # check the estimates of between-study ($\sigma^2_1$ on the following figure) and within-study variance ($\sigma^2_2$) 

## S3 method for class 'rma.mv'
profile(final_model_no_outliers_autocorrelation_matrix,
        progbar=TRUE,
        parallel="multicore")
```

Try and combine the autocorrelation matrix with the common control matrix.

```{r}
combined_matrix <- matrix(data = 0, nrow = nrow(autocorrelation_matrix), ncol = ncol(autocorrelation_matrix))

for (i in 1:nrow(autocorrelation_matrix)) {
  
    for (j in 1:ncol(autocorrelation_matrix)) {
      
      if (round(autocorrelation_matrix [i, j], digits = 12) != round(M_phylo_no_outliers [i, j], digits = 12)) {
        
        combined_matrix [i, j] <- pmax(round(autocorrelation_matrix [i, j], digits = 12),
                                       round(M_phylo_no_outliers [i, j], digits = 12))
        
      }
      
      else {
        
        combined_matrix [i, j] <- round(autocorrelation_matrix [i, j], digits = 12)
        
      }
    
  }
  
}

library(matrixcalc)
is.positive.definite(M_phylo_no_outliers)
is.positive.definite(autocorrelation_matrix) # saying it is not symmetrical. How isn't it?
is.positive.definite(round(autocorrelation_matrix, digits = 10)) # rounding error
is.positive.definite(round(combined_matrix, digits = 10)) 

View(round(combined_matrix, digits = 10))
View(t(round(combined_matrix, digits = 10)))

check_11 <- round(combined_matrix, digits = 10) == t(round(combined_matrix, digits = 10))

apply(check_11, 1, function(x) {all(x)})

```

Use this combined covariance matrix to express the dependence between the effect size estimates/sampling errors and fit the final model with no outliers. See how RVE alters the SE and CIs.

```{r}
# getting message: Warning: 'V' appears to be not positive definite.
# the results are the same though (basically) as the model above with the autocorrelation_matrix
final_model_no_outliers_combined_matrix <- rma.mv(yi = LRR, V = combined_matrix,
                    mod = ~ 0 + PESTICIDE_CLASS,
                    random = list(~1 | DATA_ID,
                                  ~1 | STUDY_ID,
                                  ~ TIME_SIMPLE | MULTIPLE_MEASUREMENT),
                    struct = "AR",
                    data = ED_50_data_phylo_no_outliers, method = "REML", test = "t", dfs = "contain")

summary(final_model_no_outliers_combined_matrix)

# are the CIs closer than before?
robust(final_model_no_outliers_combined_matrix, cluster = ED_50_data_phylo_no_outliers$STUDY_ID, clubSandwich = TRUE)
```

Publication Bias

```{r}
# repeat all the publication bias section with the no outliers dataset.

# first off a funnel plot using precision. Sub optimal approach but can identify any outliers 
# potentially due to inputting errors.
funnel(ED_50_data_phylo$LRR, ED_50_data_phylo$LRR_VAR_INDIVIDUAL, yaxis="seinv",
       #xlim = c(-3, 3),
       ylab = "Precision (1/SE)",
       xlab = "Effect size (lnRR)")

# calculating "effective sample size" to account for unbalanced sampling, for lnRR 
ED_50_data_phylo$E_N <- with(ED_50_data_phylo, (4*(AI_N_INDIVIDUAL*FORM_N_INDIVIDUAL)) / (AI_N_INDIVIDUAL + FORM_N_INDIVIDUAL))

# using effective sampling size
funnel(ED_50_data_phylo$LRR, ED_50_data_phylo$LRR_VAR_INDIVIDUAL, ni = ED_50_data_phylo$E_N, yaxis="ni",
       #xlim = c(-3, 3),
       ylab = "Effective sample size",
       xlab = "Effect size (lnRR)")

# using effective sampling size just for herbicides
funnel(ED_50_data_phylo$LRR [ED_50_data_phylo$PESTICIDE_CLASS == "herbicide"],
       ED_50_data_phylo$LRR_VAR_INDIVIDUAL [ED_50_data_phylo$PESTICIDE_CLASS == "herbicide"],
       ni = ED_50_data_phylo$E_N [ED_50_data_phylo$PESTICIDE_CLASS == "herbicide"], yaxis="ni",
       #xlim = c(-3, 3),
       ylab = "Effective sample size",
       xlab = "Effect size (lnRR)")

# preparing the moderators that need to be included in a meta-regression that also contains a  moderator with the standard errors of the effect sizes and the year of publication

# calculating the inverse of the "effective sample size" to account for unbalanced sampling, for SMD and lnRR (see Equation 25 from the main manuscript)
ED_50_data_phylo$INV_N_TILDA <-  with(ED_50_data_phylo, (AI_N_INDIVIDUAL + FORM_N_INDIVIDUAL)/(AI_N_INDIVIDUAL*FORM_N_INDIVIDUAL))
ED_50_data_phylo$SQRT_INV_N_TILDA <-  with(ED_50_data_phylo, sqrt(INV_N_TILDA))

# mean-centring year of publication to help with interpretation, particularly in S4.1.4.3.
ED_50_data_phylo$YEAR_CEN <- as.vector(scale(ED_50_data_phylo$YEAR_PUBLISHED, scale = F))

# as uncertainty increases
publication_bias_sqrt_inv_n_tilda_mod <- rma.mv(yi = LRR, V = M_phylo,
                                                mods= ~ 1 + SQRT_INV_N_TILDA,
                                                random = list(~1 | DATA_ID,
                                                              ~1 | STUDY_ID,
                                                              ~ TIME_SIMPLE | MULTIPLE_MEASUREMENT),
                                                struct = "AR",
                                                data = ED_50_data_phylo, method = "REML", test = "t", dfs = "contain")


summary(publication_bias_sqrt_inv_n_tilda_mod)

# as year increases
publication_bias_year_cen_mod <- rma.mv(yi = LRR, V = M_phylo,
                                                mods= ~ 1 + YEAR_CEN,
                                                random = list(~1 | DATA_ID,
                                                              ~1 | STUDY_ID,
                                                              ~ TIME_SIMPLE | MULTIPLE_MEASUREMENT),
                                                struct = "AR",
                                                data = ED_50_data_phylo, method = "REML", test = "t", dfs = "contain")

summary(publication_bias_year_cen_mod)


# both with PESTICIDE_CLASS. The full publication bias check.
publication_bias_all_mod <- rma.mv(yi = LRR, V = M_phylo,
                                                mods= ~ 1 + SQRT_INV_N_TILDA + YEAR_CEN + PESTICIDE_CLASS,
                                                random = list(~1 | DATA_ID,
                                                              ~1 | STUDY_ID,
                                                              ~ TIME_SIMPLE | MULTIPLE_MEASUREMENT),
                                                struct = "AR",
                                                data = ED_50_data_phylo, method = "REML", test = "t", dfs = "contain")

summary(publication_bias_all_mod)

# publication bias doesn't seem to be an issue.

# adding an interaction term again shows that the two fungicide effect sizes from the same study are massive outliers. 
publication_bias_all_mod_int <- rma.mv(yi = LRR, V = M_phylo,
                                                mods= ~ 1 + SQRT_INV_N_TILDA + YEAR_CEN + PESTICIDE_CLASS + PESTICIDE_CLASS:SQRT_INV_N_TILDA,
                                                random = list(~1 | DATA_ID,
                                                              ~1 | STUDY_ID,
                                                              ~ TIME_SIMPLE | MULTIPLE_MEASUREMENT),
                                                struct = "AR",
                                                data = ED_50_data_phylo, method = "REML", test = "t", dfs = "contain")

summary(publication_bias_all_mod_int)
```

Publication Bias Plots

```{r}
# predictions for YEAR_CEN = 0 and herbicide. No interaction term so 
# relationship the same for all PESTICIDE_CLASS levels
publication_bias_all_mod_predictions <- predict(publication_bias_all_mod,
                                                newmods=cbind(seq(min(ED_50_data_phylo$SQRT_INV_N_TILDA
                                                                      [ED_50_data_phylo$PESTICIDE_CLASS == "herbicide"]),
                                                                  max(ED_50_data_phylo$SQRT_INV_N_TILDA
                                                                   [ED_50_data_phylo$PESTICIDE_CLASS == "herbicide"]),
                                                                  length.out=324), c(0), c(0), 1, c(0)))

predictions_sqrt_inv_n_tilda <- data.frame(sint = seq(min(ED_50_data_phylo$SQRT_INV_N_TILDA 
                                                      [ED_50_data_phylo$PESTICIDE_CLASS == "herbicide"]),
                                                  max(ED_50_data_phylo$SQRT_INV_N_TILDA
                                                      [ED_50_data_phylo$PESTICIDE_CLASS == "herbicide"]),
                                                  length.out=324),
                                           PESTICIDE_CLASS = "herbicide",
                                           fit = publication_bias_all_mod_predictions$pred,
                                           upper = publication_bias_all_mod_predictions$ci.ub,
                                           lower = publication_bias_all_mod_predictions$ci.lb)

predictions_sqrt_inv_n_tilda_plot <- ggplot() +
                                     geom_point(data = ED_50_data_phylo, aes(x = SQRT_INV_N_TILDA, y = LRR)) +
                                     geom_line(data = predictions_sqrt_inv_n_tilda, aes(x = sint, y = fit, group = PESTICIDE_CLASS,
                                                                                        color = PESTICIDE_CLASS)) + 
                                     geom_ribbon(data = predictions_sqrt_inv_n_tilda, aes(x = sint, y = fit, ymin = lower, ymax = upper,
                                                                                          group = PESTICIDE_CLASS, color = PESTICIDE_CLASS, 
                                                                                          fill = PESTICIDE_CLASS), alpha = 0.3) +
                                     theme_bw() +
                                     theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank()) +
                                     scale_color_manual(values=c("#117733"), name = "Pesticide Class", labels = c("Herbicide")) +
                                     scale_fill_manual(values=c("#117733"), name = "Pesticide Class", labels = c("Herbicide")) +
                                     xlab("Square root of inverse of effective sample size") +
                                     ylab("Effect size (lnRR)") +
                                     ggtitle("Does the effect size increase in magnitude as uncertainty increases?")

predictions_sqrt_inv_n_tilda_plot

# predictions for sint = 0 and herbicide. No interaction term so 
# relationship the same for all PESTICIDE_CLASS levels
publication_bias_all_mod_predictions_year_cen <- predict(publication_bias_all_mod,
                                                newmods=cbind(mean(ED_50_data_phylo$SQRT_INV_N_TILDA
                                                                   [ED_50_data_phylo$PESTICIDE_CLASS == "herbicide"]),
                                                              seq(min(ED_50_data_phylo$YEAR_CEN
                                                                      [ED_50_data_phylo$PESTICIDE_CLASS == "herbicide"]),
                                                                  max(ED_50_data_phylo$YEAR_CEN
                                                                   [ED_50_data_phylo$PESTICIDE_CLASS == "herbicide"]),
                                                                  length.out=324),
                                                              c(0),
                                                              1,
                                                              c(0)))

predictions_year_cen <- data.frame(YEAR_PUBLISHED = seq(min(ED_50_data_phylo$YEAR_PUBLISHED 
                                                      [ED_50_data_phylo$PESTICIDE_CLASS == "herbicide"]),
                                                    max(ED_50_data_phylo$YEAR_PUBLISHED
                                                      [ED_50_data_phylo$PESTICIDE_CLASS == "herbicide"]),
                                                    length.out=324),
                                                    PESTICIDE_CLASS = "herbicide",
                                                    fit = publication_bias_all_mod_predictions_year_cen$pred,
                                                    upper = publication_bias_all_mod_predictions_year_cen$ci.ub,
                                                    lower = publication_bias_all_mod_predictions_year_cen$ci.lb)

predictions_year_cen_plot <- ggplot() +
                                     geom_point(data = ED_50_data_phylo, aes(x = YEAR_PUBLISHED, y = LRR)) +
                                     geom_line(data = predictions_year_cen, aes(x = YEAR_PUBLISHED, y = fit, group = PESTICIDE_CLASS,
                                                                                        color = PESTICIDE_CLASS)) + 
                                     geom_ribbon(data = predictions_year_cen, aes(x = YEAR_PUBLISHED, y = fit, ymin = lower, ymax = upper,
                                                                                          group = PESTICIDE_CLASS, color = PESTICIDE_CLASS, 
                                                                                          fill = PESTICIDE_CLASS), alpha = 0.3) +
                                     theme_bw() +
                                     theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank()) +
                                     scale_color_manual(values=c("#117733"), name = "Pesticide Class", labels = c("Herbicide")) +
                                     scale_fill_manual(values=c("#117733"), name = "Pesticide Class", labels = c("Herbicide")) +
                                     xlab("Year of Publication") +
                                     ylab("Effect size (lnRR)") +
                                     ggtitle("Does the effect size magnitude reduce with time?")

predictions_year_cen_plot
```

Adjusting overall effect size estimates to account for publication bias. My final model has a term where there is evidence that some groups have non-zero effects. This means I should use inverse n tilda.

```{r}
# fit model with 1/~n. Set intercept to 0 so we can see the effect on PESTICIDE_CLASS
publication_bias_all_mod_inv_n_tilda <- rma.mv(yi = LRR, V = M_phylo,
                                                mods= ~ 0 + INV_N_TILDA + YEAR_CEN + PESTICIDE_CLASS,
                                                random = list(~1 | DATA_ID,
                                                              ~1 | STUDY_ID,
                                                              ~ TIME_SIMPLE | MULTIPLE_MEASUREMENT),
                                                struct = "AR",
                                                data = ED_50_data_phylo, method = "REML", test = "t", dfs = "contain")

summary(publication_bias_all_mod_inv_n_tilda)

# preparation to get marginalized mean (when INV_N_TILDA = 0 and YEAR_CEN = 0)
# reference grid for a model object
res_publication_bias_all_mod_inv_n_tilda <- qdrg(object = publication_bias_all_mod_inv_n_tilda,
                                                 data = ED_50_data_phylo,
                                                 at = list(INV_N_TILDA = 0, YEAR_CEN = 0))


# marginalized overall mean at INV_N_TILDA = 0 and YEAR_CEN = 0; also weights = "prop" or "cells" average things over proportionally. if not specified, all groups (levels) get the same weights
mm_publication_bias_all_mod_inv_n_tilda <- emmeans(res_publication_bias_all_mod_inv_n_tilda, specs = "PESTICIDE_CLASS",
                                                            df = publication_bias_all_mod_inv_n_tilda$ddf,
                                                            weights = "prop")

# comparing results without correcting for publication bias
final_model <- rma.mv(yi = LRR, V = M_phylo,
                                                mods= ~ 0 + PESTICIDE_CLASS,
                                                random = list(~1 | DATA_ID,
                                                              ~1 | STUDY_ID,
                                                              ~ TIME_SIMPLE | MULTIPLE_MEASUREMENT),
                                                struct = "AR",
                                                data = ED_50_data_phylo, method = "REML", test = "t", dfs = "contain")



# extracting the mean and 95% confidence intervals
estimates_publication_bias_all_mod_inv_n_tilda <- data.frame(mm_publication_bias_all_mod_inv_n_tilda) [1:4, c(1:2, 5:6)]

estimates_final_model <- data.frame(final_model$beta,
                               final_model$ci.lb,
                               final_model$ci.ub)

estimate_comparison <- bind_cols(estimates_publication_bias_all_mod_inv_n_tilda,
                                 estimates_final_model)
```

"We should treat this adjusted estimate as a possible overall estimate as a part of sensitivity analysis in which we run alternative statistical models to test the robustness of results from the original analysis".

My adjusted estimates are slightly lower because there is a non-significant positive relationship between 1/~n and effect sizes. I think this is because there are relatively few studies with very small samples sizes so the distribution of effect sizes hasn't been thoroughly sampled at this study size. By chance the samples with very small sample sizes were pesticides that didn't represent the overall average across the whole sample size range. 

Post Model Fitting Checks

Makes sure that the likelihood surface around the ML/REML estimates is not flat for some combination of the parameter estimates (which would imply that the estimates are essentially arbitrary). Use profile()

```{r}
## S3 method for class 'rma.mv'
profile(final_model_no_outliers_autocorrelation_matrix,
        progbar=TRUE,
        parallel="multicore")

profile(final_model_no_outliers_combined_matrix,
        progbar=TRUE,
        parallel="multicore")
```

Imputation

plot the standard error against the ED50.

```{r}
CI_SE_combined <- read.csv("../../data_extraction/lrr_vi_full.csv")

missing_se <- read.csv("../../data_extraction/missing_se.csv")

# remove 6 effect sizes with LRR_VAR_INDIVIDUAL > 10
CI_SE_combined <- CI_SE_combined [CI_SE_combined$LRR_VAR_INDIVIDUAL < 10, ]

# ai and form with SE
se_imputation_df_ai <- cbind.data.frame(DATA_ID = CI_SE_combined$DATA_ID, ED_50 = CI_SE_combined$AI_50, SE = CI_SE_combined$AI_SE_INDIVIDUAL, FORM_AI = "AI")
se_imputation_df_form <- cbind.data.frame(DATA_ID = CI_SE_combined$DATA_ID, ED_50 = CI_SE_combined$FORM_50, SE = CI_SE_combined$FORM_SE_INDIVIDUAL, FORM_AI = "FORM")

# combine
se_imputation_df <- rbind.data.frame(se_imputation_df_ai, se_imputation_df_form)

se_imputation_df$FORM_AI <- as.factor(se_imputation_df$FORM_AI)

# plot mean vs se. SE increases with ED_50?
plot(se_imputation_df$SE ~ se_imputation_df$ED_50)

# zoom in
plot(se_imputation_df$SE [se_imputation_df$ED_50 < 2000] ~ se_imputation_df$ED_50 [se_imputation_df$ED_50 < 2000])

# zoom in
plot(se_imputation_df$SE [(se_imputation_df$ED_50 < 400) & (se_imputation_df$SE < 100)] ~ se_imputation_df$ED_50 [(se_imputation_df$ED_50 < 400) & (se_imputation_df$SE < 100)])

# relationship seems to hold.

# for the ED50s where I'm imputing their SE they will have large mean values. Therefore, they will be expected to have bigger SE?

# express se as a proportion of ed50. This controls for the large range of ED50 values and SE getting larger with ED50
# for se/ed50
# this would make it unitless too (so the non-equivalent units would not matter)
se_imputation_df$SE_PROP_ED_50 <- se_imputation_df$SE / se_imputation_df$ED_50

# plot this proportion against the ED_50
plot(se_imputation_df$SE_PROP_ED_50 ~ se_imputation_df$ED_50)

# positive relationship seems to have disappeared
plot(se_imputation_df$SE_PROP_ED_50 [se_imputation_df$ED_50 < 400] ~ se_imputation_df$ED_50 [se_imputation_df$ED_50 < 400])

# se as a proportion of mean distribution
plot(density(se_imputation_df$SE_PROP_ED_50))

# estimate the distribution the SE_PROP_ED_50 follows instead of se?
# fit a gamma distribution to the data
library(fitdistrplus)

# for < 1 add [se_imputation_df$SE_PROP_ED_50 < 1]
SE_PROP_ED_50_gamma <- fitdist(se_imputation_df$SE_PROP_ED_50,
               distr = "gamma",
               method = "mle")

summary(SE_PROP_ED_50_gamma)

# the > 1 values result in the odd qqplot.
plot(SE_PROP_ED_50_gamma)

SE_PROP_ED_50_gamma_imputation <- rgamma(n = 118, shape = SE_PROP_ED_50_gamma$estimate [[1]], rate = SE_PROP_ED_50_gamma$estimate [[2]])

min(SE_PROP_ED_50_gamma_imputation)
max(SE_PROP_ED_50_gamma_imputation)

# how many SEs do I need to impute in total? 118
nrow(rbind.data.frame(missing_se [missing_se$AI_ERROR_TYPE == "n/a", ], missing_se [missing_se$FORM_ERROR_TYPE == "n/a", ]))
```

I want to impute the missing SEs, calculate the vi and fit the final model 100 times to see how sampling from the estimated gamma distribution affects the results.

```{r}
SE_PROP_ED_50_gamma_imputation <- rgamma(n = 118, shape = SE_PROP_ED_50_gamma$estimate [[1]], rate = SE_PROP_ED_50_gamma$estimate [[2]])

# fill in n/a in missing_se with imputed values
# explanation of code below
# if error is missing fill it in with mean*imputed value
# only cycle imputed value if error_type == "n/a"
# once error blank has been filled change error type to imputed
k <- 0

for (i in 1:nrow(missing_se)) {
  
  if (missing_se$AI_ERROR_TYPE [i] == "n/a") {
    
    k <- k + 1
    
  }
  
  for (j in k:length(SE_PROP_ED_50_gamma_imputation)) {
    
    if (missing_se$AI_ERROR_TYPE [i] == "n/a") {
      
      missing_se$AI_ERROR_LOWER [i] <- missing_se$AI_50 [i] * SE_PROP_ED_50_gamma_imputation [j]
      
      missing_se$AI_ERROR_TYPE [i] <- "imputed"
      
    }
    
  }
  
}

# retaining the k value do the same of for FORM
for (i in 1:nrow(missing_se)) {
  
  if (missing_se$FORM_ERROR_TYPE [i] == "n/a") {
    
    k <- k + 1
    
  }
  
  for (j in k:length(SE_PROP_ED_50_gamma_imputation)) {
    
    if (missing_se$FORM_ERROR_TYPE [i] == "n/a") {
      
      missing_se$FORM_ERROR_LOWER [i] <- missing_se$FORM_50 [i] * SE_PROP_ED_50_gamma_imputation [j]
      
      missing_se$FORM_ERROR_TYPE [i] <- "imputed"
      
    }
    
  }
  
}

# sort out missing_se classes. NA introduced are wanted because they are replacing "n/a"
# with NA
# make AI_50 and FORM_50 numeric
missing_se$AI_50 <- as.numeric(missing_se$AI_50)
missing_se$FORM_50 <- as.numeric(missing_se$FORM_50)

# make AI_ERROR_LOWER, AI_ERROR_HIGHER, FORM_ERROR_LOWER and FORM_ERROR_HIGHER numeric
missing_se$AI_ERROR_LOWER <- as.numeric(missing_se$AI_ERROR_LOWER)
missing_se$AI_ERROR_HIGHER <- as.numeric(missing_se$AI_ERROR_HIGHER)
missing_se$FORM_ERROR_LOWER <- as.numeric(missing_se$FORM_ERROR_LOWER)
missing_se$FORM_ERROR_HIGHER <- as.numeric(missing_se$FORM_ERROR_HIGHER)

# make AI_N_CONTAINER and AI_N_INDIVIDUALS_PER_CONTAINER numeric
missing_se$AI_N_CONTAINER <- as.numeric(missing_se$AI_N_CONTAINER)
missing_se$AI_N_INDIVIDUALS_PER_CONTAINER <- as.numeric(missing_se$AI_N_INDIVIDUALS_PER_CONTAINER)

# make FORM_N_CONTAINER and FORM_N_INDIVIDUALS_PER_CONTAINER numeric
missing_se$FORM_N_CONTAINER <- as.numeric(missing_se$FORM_N_CONTAINER)
missing_se$FORM_N_INDIVIDUALS_PER_CONTAINER <- as.numeric(missing_se$FORM_N_INDIVIDUALS_PER_CONTAINER)

# use imputed values to calculate LRR_VAR_INDIVIDUAL
# add in rows to allow binding to existing dfs
missing_se$AI_BIGGEST_CI_ARM <- 0
missing_se$AI_N_INDIVIDUAL <- 0
missing_se$AI_SE_INDIVIDUAL <- 0
missing_se$AI_SE_CONTAINER <- 0

for (i in 1:nrow(missing_se)) {
  
  if ((missing_se$AI_ERROR_TYPE [i] == "imputed") | (missing_se$AI_ERROR_TYPE [i] == "SE")) {
    
    missing_se$AI_BIGGEST_CI_ARM [i] <- NA 

    missing_se$AI_N_INDIVIDUAL [i] <- missing_se$AI_N_CONTAINER [i] * missing_se$AI_N_INDIVIDUALS_PER_CONTAINER [i]
    
    missing_se$AI_SE_INDIVIDUAL [i] <- missing_se$AI_ERROR_LOWER [i]
    
  }
  
  if (missing_se$AI_ERROR_TYPE [i] == "CI_95") {
    
    if (missing_se$AI_ERROR_LOWER [i] > missing_se$AI_ERROR_HIGHER [i]) {
      
      missing_se$AI_BIGGEST_CI_ARM [i] <- missing_se$AI_ERROR_LOWER [i]
    
  }
  
    else {
    
      missing_se$AI_BIGGEST_CI_ARM [i] <- missing_se$AI_ERROR_HIGHER [i]
    
    }
    
    # add a row for AI_N_INDIVIDUAL
    missing_se$AI_N_INDIVIDUAL [i] <- missing_se$AI_N_CONTAINER [i] * missing_se$AI_N_INDIVIDUALS_PER_CONTAINER [i]
    
    missing_se$AI_SE_INDIVIDUAL [i] <- missing_se$AI_BIGGEST_CI_ARM [i] / -qt(.025, df = missing_se$AI_N_INDIVIDUAL [i] - 1)
    
  }
  
}

missing_se$FORM_BIGGEST_CI_ARM <- 0
missing_se$FORM_N_INDIVIDUAL <- 0
missing_se$FORM_SE_INDIVIDUAL <- 0
missing_se$FORM_SE_CONTAINER <- 0
missing_se$AVERAGE_CLUSTER_SIZE <- 0

for (i in 1:nrow(missing_se)) {
  
  if ((missing_se$FORM_ERROR_TYPE [i] == "imputed") | (missing_se$FORM_ERROR_TYPE [i] == "SE")) {
    
    missing_se$FORM_BIGGEST_CI_ARM [i] <- NA 

    missing_se$FORM_N_INDIVIDUAL [i] <- missing_se$FORM_N_CONTAINER [i] * missing_se$FORM_N_INDIVIDUALS_PER_CONTAINER [i]
    
    missing_se$FORM_SE_INDIVIDUAL [i] <- missing_se$FORM_ERROR_LOWER [i]
    
  }
  
  if (missing_se$FORM_ERROR_TYPE [i] == "CI_95") {
    
    if (missing_se$FORM_ERROR_LOWER [i] > missing_se$FORM_ERROR_HIGHER [i]) {
      
      missing_se$FORM_BIGGEST_CI_ARM [i] <- missing_se$FORM_ERROR_LOWER [i]
    
  }
  
    else {
    
      missing_se$FORM_BIGGEST_CI_ARM [i] <- missing_se$FORM_ERROR_HIGHER [i]
    
    }
    
    # add a row for FORM_N_INDIVIDUAL
    missing_se$FORM_N_INDIVIDUAL [i] <- missing_se$FORM_N_CONTAINER [i] * missing_se$FORM_N_INDIVIDUALS_PER_CONTAINER [i]
    
    missing_se$FORM_SE_INDIVIDUAL [i] <- missing_se$FORM_BIGGEST_CI_ARM [i] / -qt(.025, df = missing_se$FORM_N_INDIVIDUAL [i] - 1)
    
  }
  
}

missing_se$AVERAGE_CLUSTER_SIZE <- NA

missing_se$DESIGN_EFFECT_0.442_ICC <- NA

missing_se$AI_ESS_0.442_ICC <- NA

missing_se$FORM_ESS_0.442_ICC <- NA

missing_se$DESIGN_EFFECT_0.046_ICC <- NA

missing_se$AI_ESS_0.046_ICC <- NA 

missing_se$FORM_ESS_0.046_ICC <- NA

missing_se$FORM_SE_ESS_0.442 <- NA

missing_se$FORM_SE_ESS_0.046 <- NA

missing_se$AI_SE_ESS_0.442 <- NA
  
missing_se$AI_SE_ESS_0.046 <- NA

# remove rows where FORM_BIGGEST_CI_ARM or AI_BIGGEST_CI_ARM == 0 as this shows the model in the
# primary source was either not reported or not fit correctly.
missing_se <- missing_se [(missing_se$FORM_SE_INDIVIDUAL != 0) & (missing_se$AI_SE_INDIVIDUAL != 0), ]

# calculate LRR
missing_se$LRR <- log(missing_se$FORM_50/missing_se$AI_50)

# calculate LRR variance using var(lnRR) = SE_FORM^2/mean_FORM^2 + SE_AI^2/mean_AI^2
# individual SE variance
missing_se$LRR_VAR_INDIVIDUAL <- (missing_se$FORM_SE_INDIVIDUAL^2 / missing_se$FORM_50^2) + (missing_se$AI_SE_INDIVIDUAL^2 / missing_se$AI_50^2)

missing_se$LRR_VAR_CONTAINER <- NA

missing_se$LRR_VAR_CONTAINER_0.442 <- NA

missing_se$LRR_VAR_CONTAINER_0.046 <- NA

imputed_dataset <- rbind.data.frame(CI_SE_combined, missing_se)

# View(imputed_dataset [, c(1:20, 50:73)])

imputed_dataset_clean <- imputed_dataset [, c(1,2,23,32,22,3,4,5,8,12,13,14,17,24,30,33,36,42,44,45,48,51,52,55,56,69,70,49)]

# remove row where vi > 10 as the CI arm in the primary source >10 the point estimate.
imputed_dataset_clean <- imputed_dataset_clean [imputed_dataset_clean$LRR_VAR_INDIVIDUAL < 10, ]
```

Run the "full" model. LRR is now bigger as I thought it would be.

```{r}
# make TIME_SIMPLE
mult_meas_cluster <- unique(imputed_dataset_clean$MULTIPLE_MEASUREMENT)

cluster_list <- vector(mode='list', length=length(mult_meas_cluster))

for (i in 1:length(mult_meas_cluster)) {
  
    for (j in 1:nrow(imputed_dataset_clean)) {
    
    if (mult_meas_cluster [i] == imputed_dataset_clean$MULTIPLE_MEASUREMENT [j]) {
      
      cluster_list [[i]] <- rbind(cluster_list [[i]], imputed_dataset_clean [j, ])
      
    }
      
  }
  
}

for (i in 1:length(cluster_list)) {
  
  cluster_list [[i]]$TIME_SIMPLE <- 0
  
  for (j in 1:nrow(as.data.frame(cluster_list [i]))) {
    
    cluster_list [[i]][j, colnames(cluster_list [[i]]) == "TIME_SIMPLE"] <- j
    
  }
  
}

# turn back into df
library(tidyverse)

ED_50_data_simple_time <- tibble()

for (i in 1:length(cluster_list)) {
  
  ED_50_data_simple_time <- rbind(ED_50_data_simple_time, cluster_list [[i]])
  
}

# ED_50_data_simple_time$TIME_SIMPLE to imputed_dataset_clean according to DATA_ID

imputed_dataset_clean$TIME_SIMPLE <- 0

for (i in 1:nrow(imputed_dataset_clean)) {
  
  for (j in 1:nrow(ED_50_data_simple_time)) {
    
    if (imputed_dataset_clean$DATA_ID [i] == ED_50_data_simple_time$DATA_ID [j]) {
      
      imputed_dataset_clean$TIME_SIMPLE [i] <- ED_50_data_simple_time$TIME_SIMPLE [j]
      
    }
    
  }
  
}

imputed_dataset_clean <- imputed_dataset_clean [, c(1:3, 29, 4:28)]

# remove outliers
imputed_dataset_clean <- imputed_dataset_clean [(imputed_dataset_clean$DATA_ID != "409") & (imputed_dataset_clean$DATA_ID != "410"), ]

# make VCV matrix for "full" model. Just use autocorrelation matrix.
full_autocorrelation_matrix <- vcalc(vi = imputed_dataset_clean$LRR_VAR_INDIVIDUAL,
               cluster = MULTIPLE_MEASUREMENT,
               time1 = TIME_SIMPLE,
               phi=0.8,
               data=imputed_dataset_clean)

# shows the pattern I want of decreasing correlation as effect sizes become 
# further away in time.
# corrplot::corrplot(cov2cor(autocorrelation_matrix) [100:150, 100:150])

# fit the final model with this autocorrelation matrix. No warning message.
full_final_model_no_outliers_autocorrelation_matrix <- rma.mv(yi = LRR, V = full_autocorrelation_matrix,
                    mod = ~ 0 + PESTICIDE_CLASS,
                    random = list(~1 | DATA_ID,
                                  ~1 | STUDY_ID,
                                  ~ TIME_SIMPLE | MULTIPLE_MEASUREMENT),
                    struct = "AR",
                    data = imputed_dataset_clean, method = "REML", test = "t", dfs = "contain")

summary(full_final_model_no_outliers_autocorrelation_matrix)

# are the CIs closer than before?
robust(full_final_model_no_outliers_autocorrelation_matrix, cluster = imputed_dataset_clean$STUDY_ID, clubSandwich = TRUE)
```

Sensitivity analyses: With and Without Glyphosate, With and Without Imputed Data.

```{r}

```

I need to reorder everything.

Imputation should be first. Then subset dataframe using SIMULATION_REQUIRED variable.

1) Identify outliers (influential points)
i) studies where SE estimate was massive

2) Identify working model -
i) choose assumption for the correlation between sampling variances (combine common control and autoregressive correlation matrices)
ii) identify random effects structure and iii) add additional levels.

3) Fixed effects selection

4) Guard against misspecification with RVE.

1 (again)) Identify outliers (influential points)
ii) leave1out analysis (fungicide studies) including density plot

5) Explore heterogeneity

6) Explore publication bias

7) Post model fitting checks

8) Imputation

9) Sensitivity analyses
i) With and without imputed data
ii) With and without glyphosate. glyphosate alone
iii) With and without studies where regulatory guidelines were clearly followed
iv) Only look at spray treatments.
v) Adjusted for publication bias
vi) with different correlation coefficients for VCV matrix (0.1-0.9)

Potentially useful chunk.

In the chunk below I will try out the 3 ways of modelling MULTIPLE_MEASUREMENT. Using this [metafor example](https://wviechtb.github.io/metadat/reference/dat.ishak2007.html). Compare with AIC.

```{r}
# Ways of specifying models. All are the same model expressed in the different ways.
es_study_mod <- rma.mv(yi = LRR, V = M_phylo,
                    mod = ~ PESTICIDE_CLASS + EXPOSURE_ROUTE + FORMULATION_TYPE,
                    random = list(~1 | DATA_ID,
                                  ~1 | STUDY_ID),
                    data = ED_50_data_phylo, method = "REML", test = "t", dfs = "contain")

summary(es_study_mod)

i2_ml(es_study_mod)

###############

es_study_mod_1 <- rma.mv(yi = LRR, V = M_phylo,
                    mod = ~ PESTICIDE_CLASS + EXPOSURE_ROUTE + FORMULATION_TYPE,
                    random = list(~ DATA_ID | STUDY_ID),
                    data = ED_50_data_phylo, method = "REML", test = "t", dfs = "contain")

summary(es_study_mod_1)

###############

# call together to compare
# summary(es_study_mod)
 
# the models above are the same models specified in different ways to shows different things
# es_study_mod partitions the tau^2 from es_study_mod_1 into DATA_ID and STUDY_ID
# DATA_ID + STUDY_ID = tau^2
# rho in es_study_mod_1 is simply I2_STUDY_ID/I2_Total from i2_ml(es_study_mod)
# rho is the correlation coefficient between different levels of the inner grouping variable.
# which is DATA_ID, within STUDY_ID. Shows the correlation between effect sizes within a study.

###############

es_study_mod_2 <- rma.mv(yi = LRR, V = M_phylo,
                    mod = ~ PESTICIDE_CLASS + EXPOSURE_ROUTE + FORMULATION_TYPE,
                    random = list(~ 1 | STUDY_ID/DATA_ID),
                    data = ED_50_data_phylo, method = "REML", test = "t", dfs = "contain")

summary(es_study_mod_2)

###############

# call together to compare
# summary(es_study_mod)
# summary(es_study_mod_1)
# summary(es_study_mod_2)
```

-----

deleted chunk from main analysis that may become useful.

Combine the autocorrelation matrix with the common control matrix.

```{r}
combined_matrix <- matrix(data = 0, nrow = nrow(autocorrelation_matrix), ncol = ncol(autocorrelation_matrix))

for (i in 1:nrow(autocorrelation_matrix)) {
  
    for (j in 1:ncol(autocorrelation_matrix)) {
      
      if (round(autocorrelation_matrix [i, j], digits = 12) != round(common_control_matrix [i, j], digits = 12)) {
        
        combined_matrix [i, j] <- pmax(round(autocorrelation_matrix [i, j], digits = 12),
                                       round(common_control_matrix [i, j], digits = 12))
        
      }
      
      else {
        
        combined_matrix [i, j] <- round(autocorrelation_matrix [i, j], digits = 12)
        
      }
    
  }
  
}

# everything looks symmetrical
corrplot::corrplot(cov2cor(combined_matrix) [1:50, 1:50])
corrplot::corrplot(cov2cor(combined_matrix) [50:100, 50:100])
corrplot::corrplot(cov2cor(combined_matrix) [100:150, 100:150])
corrplot::corrplot(cov2cor(combined_matrix) [150:200, 150:200])
corrplot::corrplot(cov2cor(combined_matrix) [200:250, 200:250])
corrplot::corrplot(cov2cor(combined_matrix) [250:300, 250:300])
corrplot::corrplot(cov2cor(combined_matrix) [300:348, 300:348])

# when I use this combined_matrix I get an warning from metafor saying
# it isn't positive definite
library(matrixcalc)
is.positive.definite(common_control_matrix)
# is.positive.definite(autocorrelation_matrix) # saying it is not symmetrical. How isn't it?
is.positive.definite(round(autocorrelation_matrix, digits = 10)) # rounding error

# all sections of the combined matrix are postive definite apart from 100:150
is.positive.definite(round(combined_matrix [1:50, 1:50], digits = 10)) 
is.positive.definite(round(combined_matrix [50:100, 50:100], digits = 10)) 
is.positive.definite(round(combined_matrix [100:150, 100:150], digits = 10)) 
is.positive.definite(round(combined_matrix [150:200, 150:200], digits = 10)) 
is.positive.definite(round(combined_matrix [200:250, 200:250], digits = 10)) 
is.positive.definite(round(combined_matrix [250:300, 250:300], digits = 10)) 
is.positive.definite(round(combined_matrix [300:348, 300:348], digits = 10)) 

# blocks where there is symmetry but gaps throw the error
is.positive.definite(round(combined_matrix [112:120, 112:120], digits = 10)) 

# symmetry of the matrix is not an issue
symmetry_test <- round(combined_matrix, digits = 10) == t(round(combined_matrix, digits = 10))
unique(apply(symmetry_test, 1, function(x) {all(x)}))

# other properties of positive definite matrix are all eigenvalues are strictly positive
# compute eigenvalues
eigenvalues <- eigen(combined_matrix)$values

# check if all eigenvalues are positive. Some are negative so there is my issue
all_positive <- all(eigenvalues > 0)

# I am not sure whether combining two VCV matrices in the way I am doing in valid.
# probably not. I can account for both using vcalc but the implementation is hard
# look into it but for now go forward with the autocorrelation_matrix.
```